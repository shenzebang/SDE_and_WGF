{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e07b4624",
   "metadata": {},
   "source": [
    "# Gradient Flow, Class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e34dda",
   "metadata": {},
   "source": [
    "## Gradient Flow: Old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285f523",
   "metadata": {},
   "source": [
    "### Optimization problem in Euclidean space\n",
    "Let $\\mathcal{X} \\subseteq \\mathbb{R}^d$ be the ground domain of data points and let $\\mathcal{M}$ be the space of probability measures over $\\mathcal{X}$, i.e. for $\\mu\\in\\mathcal{M}$, $\\forall x\\in\\mathcal{X}, \\mu(x) \\geq 0$ and $\\int_{\\mathcal{X}} \\mu(x) \\mathrm{d} x = 1$. Consider the following optimization problem\n",
    "$$\\min_{\\theta\\in \\mathbb{R}^m} \\mathcal{L}(\\theta) = \\int_{\\mathcal{X}} \\ell(x, f_\\theta(x)) \\mu(x) \\mathrm{d} x,$$\n",
    "where $\\ell:\\mathcal{X} \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}$ is some loss function, $f_\\theta: \\mathcal{X} \\rightarrow \\mathcal{R}^m$ is some variable function parameterized by $\\theta \\in \\mathbb{R}^m$, and $\\mu \\in \\mathcal{M}(\\mathcal{X})$ is some weight measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f5039",
   "metadata": {},
   "source": [
    "### Gradient in Euclidean space\n",
    "Let us recall the definition of the gradient of a function $\\mathcal{L}$ with respect to the variable $\\theta$: \n",
    "Suppose that given $\\theta_0 \\in \\mathbb{R}^m$, the following equation holds for any $\\delta \\theta \\in \\mathbb{R}^{m}$\n",
    "$$\\lim_{\\epsilon\\rightarrow 0} \\frac{1}{\\epsilon}|\\mathcal{L}(\\theta + \\epsilon\\cdot\\delta\\theta) - \\mathcal{L}(\\theta) - \\epsilon \\langle \\nabla_{\\theta} \\mathcal{L}_{\\vert \\theta = \\theta_0}, \\delta\\theta)|  = 0.$$\n",
    "We call $\\nabla_{\\theta} \\mathcal{L}_{\\vert \\theta = \\theta_0}$ the gradient of $\\mathcal{L}$ w.r.t. $\\theta$ at $\\theta = \\theta_0$. For the simplicity of notations, we denote it by $\\nabla \\mathcal{L}(\\theta_0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d726d",
   "metadata": {},
   "source": [
    "### (optional) Alternative definition of the gradient in Euclidean space\n",
    "Given $\\theta_0 \\in \\mathbb{R}^m$, suppose that for any differentiable trajectory $\\{\\theta(t)\\}_{t\\in[- \\delta t, + \\delta t]}$ that satisfies $\\theta(0) = \\theta_0$, the following equation holds\n",
    "$$\\frac{\\mathrm{d}}{\\mathrm{d} t} \\mathcal{L}(\\theta(t))_{\\vert t=0} = \\langle \\nabla \\mathcal{L}(\\theta_0), \\dot \\theta(0) \\rangle.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a49955",
   "metadata": {},
   "source": [
    "### Gradient flow in the Euclidean space (EGF)\n",
    "Our goal is to find a stationary point of the minimization problem $\\min_\\theta \\mathcal{L}(\\theta)$ which is to solve the static equation\n",
    "$$\\nabla \\mathcal{L}(\\theta) = 0.$$\n",
    "The gradient flow is the following dynamic equation that satisfies the above static equation at its equilibrium\n",
    "$$\n",
    "\\frac{\\mathrm{d}}{\\mathrm{d} t} \\theta(t) = - \\nabla \\mathcal{L}(\\theta(t)),\\ \\mathrm{with}\\ \\theta(0) = \\theta_0.\n",
    "$$\n",
    "It is the continuous limit of the well-known gradient descent method.\n",
    "#### (optional) Key ingredients of a gradient flow\n",
    "From the above gradient flow in the Euclidean space, it may seems that to define a gradient flow, the only ingradient that we need is the objective function $\\mathcal{L}$. However, this is only true in this special case since the inner product (or more generally the norm) of the Euclidean space is given a priori. \n",
    "\n",
    "In fact, to define a gradient flow, there two key ingradients:\n",
    "1. the objective function ($\\mathcal{L}$ in the EGF case) and \n",
    "2. the inner product structure (or more generally the norm structure) of the underlying space of the variable ($\\mathbb{R}^m$ in the EGF case).\n",
    "\n",
    "In the following (and in the rest of the course), we will elaborate on examples where the inner product structure of the underlying space is not so obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f633038c",
   "metadata": {},
   "source": [
    "#### An concrete example of the gradient flow in the Euclidean space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba6b8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b36148",
   "metadata": {},
   "source": [
    "### Heat equation\n",
    "We now introduce the heat equation (HE), which will be the key example of our mini course,\n",
    "$$\n",
    "u_t = \\alpha^2 \\Delta u.\n",
    "$$\n",
    "Here $u:\\mathbb{R}^d \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ is a function of space and time, $u_t$ is the shorthand for $\\frac{\\partial u}{\\partial t}$ and $\\Delta$ stands for Laplacian operator, i.e. \n",
    "$$\n",
    "\\Delta = \\sum_{i=1}^d \\frac{\\partial^2}{\\partial x_i^2}.\n",
    "$$.\n",
    "\n",
    "We will show that HE is the gradient flow of the Dirichlet energy in the $\\mathcal{L}^2(\\mathbb{R}^d)$ function space and it is also the gradient flow of the entropy in the Wasserstein-2 space. This is a great example that shows a gradient flow is jointly determined by the objective function(al) and the product structure (or more generally the norm structure) of the underlying space of the variable.\n",
    "\n",
    "Don't worry that you are not familiar with these concepts as they will be described in details in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3f2bc",
   "metadata": {},
   "source": [
    "#### (optional) Derive 1D HE from basic laws of physics\n",
    "Let $u(x, t)$ be the temperature in a rod of infinite length ($d=1$). Moreover, suppose that the rod that satisfies the following assumptions:\n",
    "1. The rod is made of a homogeneous material.\n",
    "2. The rod is laterally insulated, so that heat flows only in the x-direction.\n",
    "3. The rod is sufficiently thin so that the temperature within any particular cross-section is constant.\n",
    "\n",
    "From the principle of conservation of energy, it follows that the heat within a segment of the rod $[x, x+\\Delta x]$ satisfies the following:\n",
    "$$\n",
    "\\text{Net change inside }[x, x+\\Delta x] = \\text{Net inward flux across boundaries}.\n",
    "$$\n",
    "The total amount of heat, in calories, in any segment $[a, b]$ is given by $\\int_{a}^{b} c \\rho A u(s, t) \\mathrm{d} s$, where $c$ is the thermal capacity of the rod (also known as the specific heat), $\\rho$ is the density of the rod, and $A$ is the cross-sectional area of the rod. In view of our assumptions, $c$, $ρ$ and $A$ are constants. Also, recall that the flux from left to right at $x = a$ is given by $−ku_x(a,t)$, where $k$ is the thermal conductivity of the rod and $u_x$ is the shorthand for $\\frac{\\partial u}{\\partial x}$. \n",
    "\n",
    "Putting all of these facts together, we can translate the conservation relation into the equation\n",
    "$$\n",
    "c\\rho A \\int_x^{x + \\Delta x} u_t(s, t) \\mathrm{d} s = k A[u_x(x+\\Delta x, t) - u_x(x, t)].\n",
    "$$\n",
    "Applying the Fundamental Theorem of Calculus “in reverse” $f(b) - f(a) = \\int_a^b f'(s) \\mathrm{d} s$, we obtain\n",
    "$$\n",
    "c\\rho A \\int_x^{x + \\Delta x} u_t(s, t) - \\alpha^2 u_{xx}(s, t) \\mathrm{d} s = 0,\n",
    "$$\n",
    "where $\\alpha^2 = \\frac{k}{c\\rho}$, is the diffusivity of the rod.\n",
    "\n",
    "Since this equation holds on an arbitrary segment of the rod, it follows that the integrand must vanish everywhere in the rod, which yield the equation\n",
    "$$\n",
    "u_t = \\alpha^2 u_{xx}.\n",
    "$$\n",
    "\n",
    "\n",
    "Credit: The derivation is borrowed from https://www.math.usm.edu/lambers/mat417/spr14/lecture3.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b320ac1",
   "metadata": {},
   "source": [
    "### HE is the gradient flow of the Dirichlet energy in the $\\mathcal{L}^2(\\mathbb{R}^d)$ function space\n",
    "Define the Dirichlet energy as\n",
    "$$\n",
    "\\mathcal{L}_{dir}(u) = \\frac{\\alpha^2}{2}\\int_{\\mathbb{R}^d} \\|\\nabla u(x)\\|^2 \\mathrm{d} x,\n",
    "$$\n",
    "where $\\nabla = [\\frac{\\partial}{\\partial x_1}, \\cdots, \\frac{\\partial}{\\partial x_d}]$ is the gradient operator.\n",
    "And define the $\\mathcal{L}^2(\\mathbb{R}^d)$ function space to be the collection of square-integrable functions, i.e.\n",
    "$$\n",
    "\\mathcal{L}^2(\\mathbb{R}^d) = \\{ f:\\mathbb{R}^d\\rightarrow \\mathbb{R} | \\int_{\\mathbb{R}^d} |f(x)|^2 \\mathrm{d} x < \\infty\\}.\n",
    "$$\n",
    "We now show that HE is the graident flow of the Dirichlet energy $\\mathcal{L}_{dir}$ in the $\\mathcal{L}^2(\\mathcal{M})$ function space.\n",
    "#### $\\mathcal{L}^2(\\mathbb{R}^d)$ function space\n",
    "The $\\mathcal{L}^2(\\mathbb{R}^d)$ function space is a Hilbert space with the inner product structure\n",
    "$$\n",
    "\\langle f, g\\rangle_{\\mathcal{L}^2} = \\int_{\\mathbb{R}^d} f(x)g(x) \\mathrm{d} x,\n",
    "$$\n",
    "for $f, g \\in \\mathcal{L}^2(\\mathbb{R}^d)$.\n",
    "In the following, we omit the subscript $\\mathcal{L}^2$ for the simplicity of notations.\n",
    "#### Functional gradient\n",
    "\n",
    "\n",
    "#### Dirichlet energy and HE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6fd64",
   "metadata": {},
   "source": [
    "#### An concrete example of HE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb671a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59469847",
   "metadata": {},
   "source": [
    "## Gradient Flow: New"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ee3a9d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fcfc7d8",
   "metadata": {},
   "source": [
    "## Outline of the Course\n",
    "1. A more detailed review of the EGF (GF: Old)\n",
    "2. Key theorems of the Wasserstein metric (GF: New)\n",
    "    1. Brenier’s Theorem\n",
    "    2. Wasserstein distance is a distance\n",
    "    3. McCann’s Interpolation\n",
    "    4. Benamou-Brenier’s formula\n",
    "3. Derive WGF from the JKO scheme. Show example PDEs that are WGF\n",
    "4. Analyze the regularity & convergence of example PDEs, FPE & 2D NSE. Derive NN based solvers from the analysis. Show some experimental results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a70e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
