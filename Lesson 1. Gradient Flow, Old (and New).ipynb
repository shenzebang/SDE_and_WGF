{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e07b4624",
   "metadata": {},
   "source": [
    "# Gradient Flow, Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e34dda",
   "metadata": {},
   "source": [
    "## Gradient Flow: Old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285f523",
   "metadata": {},
   "source": [
    "### Optimization problem in Euclidean space\n",
    "Let $\\mathcal{X} \\subseteq \\mathbb{R}^d$ be the ground domain of data points and let $\\mathcal{M}$ be the space of probability measures over $\\mathcal{X}$, i.e. for $\\mu\\in\\mathcal{M}$, $\\forall x\\in\\mathcal{X}, \\mu(x) \\geq 0$ and $\\int_{\\mathcal{X}} \\mu(x) \\mathrm{d} x = 1$. Consider the following optimization problem\n",
    "$$\n",
    "\\min_{\\theta\\in \\mathbb{R}^m} \\mathcal{L}(\\theta) = \\int_{\\mathcal{X}} \\ell(x, f_\\theta(x)) \\mu(x) \\mathrm{d} x,\n",
    "$$\n",
    "where $\\ell:\\mathcal{X} \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ is some loss function, $f_\\theta: \\mathcal{X} \\rightarrow \\mathbb{R}$ is some variable function parameterized by $\\theta \\in \\mathbb{R}^m$, and $\\mu \\in \\mathcal{M}(\\mathcal{X})$ is some weight measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f5039",
   "metadata": {},
   "source": [
    "### Gradient in Euclidean space\n",
    "Let us recall the definition of the gradient of a function $\\mathcal{L}$ with respect to the variable $\\theta$: \n",
    "Suppose that given $\\theta_0 \\in \\mathbb{R}^m$, the following equation holds for any $\\delta \\theta \\in \\mathbb{R}^{m}$\n",
    "$$\n",
    "\\lim_{\\epsilon\\rightarrow 0} \\frac{1}{\\epsilon}|\\mathcal{L}(\\theta_0 + \\epsilon\\cdot\\delta\\theta) - \\mathcal{L}(\\theta_0) - \\epsilon \\langle \\nabla_{\\theta} \\mathcal{L}_{\\vert \\theta = \\theta_0}, \\delta\\theta)|  = 0.\n",
    "$$\n",
    "We call $\\nabla_{\\theta} \\mathcal{L}_{\\vert \\theta = \\theta_0}$ the gradient of $\\mathcal{L}$ w.r.t. $\\theta$ at $\\theta = \\theta_0$. For the simplicity of notations, we denote it by $\\nabla \\mathcal{L}(\\theta_0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d726d",
   "metadata": {},
   "source": [
    "### (optional) Alternative definition of the gradient in Euclidean space\n",
    "Given $\\theta_0 \\in \\mathbb{R}^m$, suppose that for any differentiable trajectory $\\{\\theta(t)\\}_{t\\in[- \\delta t, + \\delta t]}$ that satisfies $\\theta(0) = \\theta_0$, the following equation holds\n",
    "$$\\frac{\\mathrm{d}}{\\mathrm{d} t} \\mathcal{L}(\\theta(t))_{\\vert t=0} = \\langle \\nabla \\mathcal{L}(\\theta_0), \\dot \\theta(0) \\rangle.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a49955",
   "metadata": {},
   "source": [
    "### Gradient flow in the Euclidean space (EGF)\n",
    "Our goal is to find a stationary point of the minimization problem $\\min_\\theta \\mathcal{L}(\\theta)$ which is to solve the static equation\n",
    "$$\\nabla \\mathcal{L}(\\theta) = 0.$$\n",
    "The gradient flow is the following dynamic equation that satisfies the above static equation at its equilibrium\n",
    "$$\n",
    "\\frac{\\mathrm{d}}{\\mathrm{d} t} \\theta(t) = - \\nabla \\mathcal{L}(\\theta(t)),\\ \\mathrm{with}\\ \\theta(0) = \\theta_0.\n",
    "$$\n",
    "It is the continuous limit of the well-known gradient descent method.\n",
    "\n",
    "#### (optional) Connect EGF with gradient descent (informal)\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_{k} - \\eta \\nabla \\mathcal{L}(\\theta_k) \\Leftrightarrow \\frac{\\theta_{k+1} - \\theta_{k}}{\\eta} = - \\nabla \\mathcal{L}(\\theta_k).\n",
    "$$\n",
    "Consider a collection of curves $\\theta(t; \\eta)$ indexed by $\\eta \\geq 0$ such that for $t = a\\cdot k\\eta + (1-a)\\cdot (k+1)\\eta, a \\in [0, 1]$, $\\theta(t; \\eta) = a\\cdot \\theta_k + (1-a)\\cdot \\theta_{k+1}$. \n",
    "Denote $\\Delta t = \\eta$.\n",
    "$$\n",
    "\\frac{\\mathrm{d} \\theta(t; 0)}{\\mathrm{d} t}_{\\vert t = t_0} = \\lim_{\\Delta t \\rightarrow 0} \\frac{\\theta(t_0+\\Delta t; \\Delta t) - \\theta(t_0; \\Delta t)}{\\Delta t} = -\\nabla \\mathcal{L}(\\theta(t_0; 0)).\n",
    "$$\n",
    "\n",
    "\n",
    "#### (optional) Key ingredients of a gradient flow\n",
    "From the above gradient flow in the Euclidean space, it may seems that to define a gradient flow, the only ingradient that we need is the objective function $\\mathcal{L}$. However, this is only true in this special case since the inner product (or more generally the norm) of the Euclidean space is given a priori. \n",
    "\n",
    "In fact, to define a gradient flow, there two key ingradients:\n",
    "1. the objective function ($\\mathcal{L}$ in the EGF case) and \n",
    "2. the inner product structure (or more generally the norm structure) of the underlying space of the variable ($\\mathbb{R}^m$ in the EGF case).\n",
    "\n",
    "In the following (and in the rest of the course), we will elaborate on examples where the inner product structure of the underlying space is not so obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f633038c",
   "metadata": {},
   "source": [
    "#### An concrete example of the gradient flow in the Euclidean space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba6b8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b36148",
   "metadata": {},
   "source": [
    "### Heat equation\n",
    "We now introduce the heat equation (HE), which will be the key example of our mini course,\n",
    "$$\n",
    "u_t = \\alpha^2 \\Delta u.\n",
    "$$\n",
    "Here $u:\\mathbb{R}^d \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ is a function of space and time, $u_t$ is the shorthand for $\\frac{\\partial u}{\\partial t}$ and $\\Delta$ stands for Laplacian operator, i.e. \n",
    "$$\n",
    "\\Delta = \\sum_{i=1}^d \\frac{\\partial^2}{\\partial x_i^2} = \\mathrm{div} \\cdot \\nabla.\n",
    "$$\n",
    "Here $\\mathrm{div} = \\sum_{i=1}^d \\frac{\\partial}{\\partial x_i}$ denotes the divergence operator and $\\nabla  = \\begin{bmatrix} \n",
    "\\frac{\\partial}{\\partial x_1} \\\\\n",
    "\\cdots \\\\\n",
    "\\frac{\\partial}{\\partial x_d} \n",
    "\\end{bmatrix}.$ stands for the gradient operator.\n",
    "\n",
    "We will show that HE is the gradient flow of the Dirichlet energy in the $\\mathcal{L}^2(\\mathbb{R}^d)$ function space and it is also the gradient flow of the entropy in the Wasserstein-2 space. This is a great example that shows a gradient flow is jointly determined by the objective function(al) and the product structure (or more generally the norm structure) of the underlying space of the variable.\n",
    "\n",
    "Don't worry that you are not familiar with these concepts as they will be described in details in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3f2bc",
   "metadata": {},
   "source": [
    "#### (optional) Derive 1D HE from basic laws of physics\n",
    "Let $u(x, t)$ be the temperature in a rod of infinite length ($d=1$). Moreover, suppose that the rod that satisfies the following assumptions:\n",
    "1. The rod is made of a homogeneous material.\n",
    "2. The rod is laterally insulated, so that heat flows only in the x-direction.\n",
    "3. The rod is sufficiently thin so that the temperature within any particular cross-section is constant.\n",
    "\n",
    "From the principle of conservation of energy, it follows that the heat within a segment of the rod $[x, x+\\Delta x]$ satisfies the following:\n",
    "$$\n",
    "\\text{Net change inside }[x, x+\\Delta x] = \\text{Net inward flux across boundaries}.\n",
    "$$\n",
    "The total amount of heat, in calories, in any segment $[a, b]$ is given by $\\int_{a}^{b} c \\rho A u(s, t) \\mathrm{d} s$, where $c$ is the thermal capacity of the rod (also known as the specific heat), $\\rho$ is the density of the rod, and $A$ is the cross-sectional area of the rod. In view of our assumptions, $c$, $ρ$ and $A$ are constants. Also, recall that the flux from left to right at $x = a$ is given by $−ku_x(a,t)$, where $k$ is the thermal conductivity of the rod and $u_x$ is the shorthand for $\\frac{\\partial u}{\\partial x}$. \n",
    "\n",
    "Putting all of these facts together, we can translate the conservation relation into the equation\n",
    "$$\n",
    "c\\rho A \\int_x^{x + \\Delta x} u_t(s, t) \\mathrm{d} s = k A[u_x(x+\\Delta x, t) - u_x(x, t)].\n",
    "$$\n",
    "Applying the Fundamental Theorem of Calculus “in reverse” $f(b) - f(a) = \\int_a^b f'(s) \\mathrm{d} s$, we obtain\n",
    "$$\n",
    "c\\rho A \\int_x^{x + \\Delta x} u_t(s, t) - \\alpha^2 u_{xx}(s, t) \\mathrm{d} s = 0,\n",
    "$$\n",
    "where $\\alpha^2 = \\frac{k}{c\\rho}$, is the diffusivity of the rod.\n",
    "\n",
    "Since this equation holds on an arbitrary segment of the rod, it follows that the integrand must vanish everywhere in the rod, which yield the equation\n",
    "$$\n",
    "u_t = \\alpha^2 u_{xx}.\n",
    "$$\n",
    "\n",
    "\n",
    "Credit: The derivation is borrowed from https://www.math.usm.edu/lambers/mat417/spr14/lecture3.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b320ac1",
   "metadata": {},
   "source": [
    "### HE is the gradient flow of the Dirichlet energy in the $\\mathcal{L}^2(\\mathbb{R}^d)$ function space\n",
    "Define the Dirichlet energy as\n",
    "$$\n",
    "\\mathcal{L}_{dir}(u) = \\frac{\\alpha^2}{2}\\int_{\\mathbb{R}^d} \\|\\nabla u(x)\\|^2 \\mathrm{d} x,\n",
    "$$\n",
    "where $\\nabla = [\\frac{\\partial}{\\partial x_1}, \\cdots, \\frac{\\partial}{\\partial x_d}]$ is the gradient operator.\n",
    "And define the $\\mathcal{L}^2(\\mathbb{R}^d)$ function space to be the collection of square-integrable functions, i.e.\n",
    "$$\n",
    "\\mathcal{L}^2(\\mathbb{R}^d) = \\{ f:\\mathbb{R}^d\\rightarrow \\mathbb{R} | \\int_{\\mathbb{R}^d} |f(x)|^2 \\mathrm{d} x < \\infty\\}.\n",
    "$$\n",
    "We now show that HE is the graident flow of the Dirichlet energy $\\mathcal{L}_{dir}$ in the $\\mathcal{L}^2(\\mathcal{M})$ function space.\n",
    "#### $\\mathcal{L}^2(\\mathbb{R}^d)$ function space\n",
    "The $\\mathcal{L}^2(\\mathbb{R}^d)$ function space is a Hilbert space with the inner product structure\n",
    "$$\n",
    "\\langle f, g\\rangle_{\\mathcal{L}^2} = \\int_{\\mathbb{R}^d} f(x)g(x) \\mathrm{d} x,\n",
    "$$\n",
    "for $f, g \\in \\mathcal{L}^2(\\mathbb{R}^d)$.\n",
    "In the following, we omit the subscript $\\mathcal{L}^2$ for the simplicity of notations.\n",
    "#### Functional gradient\n",
    "For $f\\in \\mathcal{L}^2$, consider an objective functional $\\mathcal{L}:\\mathcal{L}^2 \\rightarrow \\mathbb{R}$.\n",
    "We say $\\mathcal{L}[f]$ is Frechet differentiable if at $f_0$ is there exists a bounded linear operator $A:\\mathcal{L}^2\\rightarrow \\mathbb{R}$ such that\n",
    "$$\n",
    "\\lim_{\\|h\\|_{\\mathcal{L}^2}\\rightarrow 0} \\frac{|\\mathcal{L}[f_0 + h] - \\mathcal{L}[f_0] - A[h]|}{\\|h\\|_{\\mathcal{L^2}}} = 0.\n",
    "$$\n",
    "Since the $\\mathcal{L}^2$ space is a Hilbert space, according to the Riesz representation theorem, we know that the linear operator $A$ can be represented as $A[h] = \\langle g, h\\rangle$ where $g \\in \\mathcal{L}^2$.\n",
    "We call $g$ the functional gradient and denote it by $\\frac{\\delta \\mathcal{L}}{\\delta f}[f_0]$.\n",
    "\n",
    "Let us consider the following objective functional\n",
    "$$\n",
    "\\mathcal{L}[f] = \\int \\ell(x, f(x), \\nabla f(x)) \\mathrm{d} x.\n",
    "$$\n",
    "The functional gradient can be computed as follows\n",
    "\\begin{align}\n",
    "\\mathcal{L}[f + h] - \\mathcal{L}[f] =&\\ \\int \\ell(x, [f+h](x), \\nabla [f+h](x)) \\mathrm{d} x - \\int \\ell(x, f(x), \\nabla f(x)) \\mathrm{d} x \\\\\n",
    "=&\\ \\int \\frac{\\partial \\ell}{\\partial f(x)} h(x) + \\frac{\\partial \\ell}{\\partial \\nabla f(x)} \\nabla h(x) \\mathrm{d} x \\\\\n",
    "=&\\ \\int \\frac{\\partial \\ell}{\\partial f(x)} h(x) - \\mathrm{div} \\cdot \\frac{\\partial \\ell}{\\partial \\nabla f(x)} h(x) \\mathrm{d} x \\\\\n",
    "=&\\ \\int \\left( \\frac{\\partial \\ell}{\\partial f(x)} - \\mathrm{div} \\cdot \\frac{\\partial \\ell}{\\partial \\nabla f(x)}\\right) h(x) \\mathrm{d} x.\n",
    "\\end{align}\n",
    "Consequently, we can compute that\n",
    "\\begin{equation}\n",
    "\\frac{\\delta \\mathcal{L}}{\\delta f}[f_0] = \\frac{\\partial \\ell}{\\partial f(x)} - \\mathrm{div} \\cdot \\frac{\\partial \\ell}{\\partial \\nabla f(x)}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "#### Dirichlet energy and HE\n",
    "We now compute the functional gradient of the Dirichlet energy $\\mathcal{L}_{dir}$ in the $\\mathcal{L}^2$ space as follows:\n",
    "\\begin{equation}\n",
    "\\frac{\\delta \\mathcal{L}}{\\delta f}[f_0] = - \\alpha^2 \\mathrm{div} \\cdot \\nabla f = - \\alpha^2\\Delta f.\n",
    "\\end{equation}\n",
    "Hence, we recover the heat equation as the gradient flow of the Dirichlet energy $\\mathcal{L}_{dir}$ in the $\\mathcal{L}^2$ space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6fd64",
   "metadata": {},
   "source": [
    "#### An concrete example of HE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c8b07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59469847",
   "metadata": {},
   "source": [
    "## Gradient Flow: New"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ee3a9d",
   "metadata": {},
   "source": [
    "In the previous sections, we briefly discussed some classic results of the gradient flow: 1. we reviewed the gradient flow in the Euclidean space (i.e. a finite dimensional Hilbert space) and 2. we showed that the classic heat equation is the gradient flow of the Dirichlet energy in the $\\mathcal{L}^2$ function space. \n",
    "\n",
    "We now briefly mention the renovated interest of the notaion of the gradient initialized by \\cite{?} and \\cite{?} (JKO), where they observe that PDEs like\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\rho}{\\partial t} + \\mathrm{div} \\cdot \\left( \\rho \\cdot - \\nabla \\frac{\\partial \\mathcal{F}}{\\partial \\rho} \\right) = 0,\\ \\rho(0, \\cdot) = \\rho_0\n",
    "\\end{equation}\n",
    "are actually gradient flows of the objective funtional $\\mathcal{F}$ in the Wasserstein-2 space, given that the initial data $\\rho_0$ is a probability distribution.\n",
    "\n",
    "#### HE is the gradient flow of the entroy in the Wasserstein-2 space\n",
    "Let us consider the following concrete example of the objective functional \n",
    "\\begin{equation}\n",
    "\\mathcal{F}(\\rho) =  \\alpha^2 \\int \\rho(x) \\log \\rho(x) \\mathrm{d} x.\n",
    "\\end{equation}\n",
    "One can compute that\n",
    "\\begin{equation}\n",
    "\\frac{\\delta \\mathcal{F}}{\\delta \\rho} = \\alpha^2( \\log \\rho(x) + 1).\n",
    "\\end{equation}\n",
    "This leads to the following gradient flow in the Wasserstein-2 space\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\rho}{\\partial t} + \\mathrm{div} \\cdot \\left( \\rho \\cdot - \\alpha^2 \\nabla \\log \\rho \\right) = \\frac{\\partial \\rho}{\\partial t} - \\alpha^2 \\Delta \\rho = 0,\n",
    "\\end{equation}\n",
    "which is exactly HE.\n",
    "Therefore HE is a gradient flow, DOUBLE TIMES!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcfc7d8",
   "metadata": {},
   "source": [
    "## Outline of the Course\n",
    "1. A more detailed review of the EGF (GF: Old)\n",
    "2. Key theorems of the Wasserstein metric (GF: New)\n",
    "    1. Brenier’s Theorem\n",
    "    2. Wasserstein distance is a distance\n",
    "    3. McCann’s Interpolation\n",
    "    4. Benamou-Brenier’s formula\n",
    "3. Derive WGF from the JKO scheme. Show example PDEs that are WGF\n",
    "4. Analyze the regularity & convergence of example PDEs, FPE & 2D NSE. Derive NN based solvers from the analysis. Show some experimental results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a70e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
